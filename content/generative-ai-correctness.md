# Generative AI Correctness

When prompting AI to generate text (via ChatGPT, Google Bard, etc.), the output can *appear* correct by being rationally constructed and well-formed. However, the assertions can still be false.

This has an interesting analogue in human interactions. An assertion may be proposed, and the internal components may all be logically consistent and coherent, but because some subtle nuance is incorrect, the results will be incorrect.

How do we solve for this in human interactions? Can that solve for AI interactions as well?